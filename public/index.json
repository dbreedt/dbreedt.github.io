[{"content":"The rapid advancements in quantum computing research have positioned it as both a force for innovation and a looming threat to the financial sector. While some celebrate groundbreaking discoveries, others amplify concerns over the risks quantum computing may pose to financial institutions.\nThe United Nations has declared 2025 the International Year of Quantum Science and Technology , and experts speculate about the so-called ‚ÄòQ-Day‚Äô ‚Äîthe moment when quantum computers will be capable of breaking the cryptographic systems that secure digital communications and transactions.\nFinancial institutions are already preparing for this shift. HSBC, in collaboration with IBM , anticipates that the financial services industry will be among the first to be transformed by quantum computing and Artificial Intelligence (AI)‚Äîa convergence now referred to as AQ. Echoing this view, AI and quantum computing were key discussion points at the Singapore FinTech Festival in November 2024.\nWhile quantum computing itself is not new, the excitement around its potential to go mainstream within the next few years is growing.\nOpportunities for the Financial Sector AQ offers exponentially faster data processing, which could transform risk mitigation, fraud detection, and financial modelling. By enhancing machine learning, it has the potential to reshape:\nRisk estimation and trading strategies ‚Äì uncovering hidden\ncorrelations in vast datasets to improve market predictions.\nCustomer analytics and personalisation ‚Äì enabling banks, insurers,\nand investment firms to provide smarter, AI-driven financial advice tailored to individual risk profiles.\nReal-time risk analysis and portfolio optimisation ‚Äì allowing for\nmore precise decision-making.\nInsurance ‚Äì improving risk assessment, dynamic pricing, and fraud\ndetection, leading to hyper-personalised policies with greater predictive accuracy.\nThis technological leap is expected to drive efficiency and innovation across the financial sector.\nRisks for the Financial Sector Quantum computing also introduces significant risks, particularly in cybersecurity. Algorithms such as Shor\u0026rsquo;s Algorithm and Grover\u0026rsquo;s Algorithm could undermine the cryptographic protections that currently safeguard digital communications and transactions:\nShor\u0026rsquo;s Algorithm enables polynomial-time factorisation of large\nintegers, threatening RSA and Elliptic Curve Cryptography (ECC)‚Äîwhich underpin authentication, transaction verification, and secure communications.\nGrover‚Äôs Algorithm accelerates brute-force attacks on symmetric\nencryption, reducing the effective key length of encryption standards like AES.\nFor financial institutions, the potential impact extends beyond encryption:\nCompromised authentication mechanisms ‚Äì attackers could\nimpersonate systems, manipulate transaction records, or gain unauthorised access to sensitive data.\nVulnerabilities in cryptocurrencies ‚Äì Bitcoin wallets, which rely\non ECC, could be at risk if a sufficiently powerful quantum computer derives private keys from public ones.\nAlthough quantum computers could theoretically break these encryption methods, doing so would require fault-tolerant quantum machines with millions of stable qubits . Currently, the most advanced quantum computers operate with only a few thousand noisy qubits, making these threats largely theoretical for now.\nThe Current State of Quantum Computing Despite significant progress , quantum computing remains far from mainstream adoption. Major challenges include:\nQubit stability and error correction ‚Äì as quantum computers scale,\nmaintaining coherence becomes increasingly difficult.\nExtreme operating conditions ‚Äì most systems require temperatures\nnear absolute zero.\nThe need for millions of qubits ‚Äì breaking RSA-2048 encryption,\nfor example, is estimated to require around 6,190 logical qubits, translating into millions of physical qubits when accounting for error correction.\nWhile research continues at a rapid pace, many experts believe it will take a decade before quantum computers can reliably break classical encryption.\nLooking Ahead Quantum computing is advancing, but mainstream adoption remains distant. Several breakthroughs‚Äîsuch as room-temperature superconductors , mechanical qubits , or Google‚Äôs new Willow chip ‚Äîcould help overcome existing limitations. For now, access to quantum computing remains expensive and largely limited to cloud-based services .\nMeanwhile, cryptographic experts are actively developing post-quantum cryptographic standards to mitigate potential threats. Industry commentary suggests that most financial institutions will likely be quantum-immune before quantum computers become a mainstream security risk.\nAs quantum computing matures, its impact is likely to be transformative, much like GPUs and AI have been‚Äîrevolutionising some tasks while serving as an additional tool for others.\nFinal Thoughts Quantum computing holds immense promise for the financial sector, offering both opportunities and risks. While it is not yet ready to disrupt global financial systems, the race to prepare for its eventual rise has already begun. Institutions that invest in quantum-readiness today will be best positioned to navigate the future of finance.\n","permalink":"https://dbreedt.github.io/posts/quantum-computing-finance/","summary":"\u003cp\u003eThe rapid advancements in quantum computing research have positioned it\nas both a force for innovation and a looming threat to the financial\nsector. While some celebrate groundbreaking discoveries, others amplify\nconcerns over the risks quantum computing may pose to financial\ninstitutions.\u003c/p\u003e\n\u003cp\u003eThe United Nations has declared 2025 the \u003ca href=\"https://quantum2025.org/\"\n   \n    \n      target=\"_blank\" rel=\"noopener\" \n   \u003e\n   \u003cem\u003eInternational Year of Quantum\nScience and Technology\u003c/em\u003e\n\u003c/a\u003e\n, and experts speculate about the\nso-called \u003ca href=\"https://www.cigionline.org/articles/q-day-is-coming-is-the-world-prepared/\"\n   \n    \n      target=\"_blank\" rel=\"noopener\" \n   \u003e\n   ‚ÄòQ-Day‚Äô\n\u003c/a\u003e\n‚Äîthe\nmoment when quantum computers will be capable of breaking the cryptographic systems that secure digital\ncommunications and transactions.\u003c/p\u003e","title":"Quantum Computing in Finance: A Game-Changer... Eventually"},{"content":"An adage goes: ‚ÄúThe more things change, the more they stay the same‚Äù ‚Äî that kind of holds true for Software Development.\nBack in the day, we used to design our systems, write the code, compile it, run it, get it tested and then deploy it. We still do all of that today, regardless of what we are building, be it microservices, large scale monolith systems, mobile apps, websites or firmware for IOT things.\nSo nothing has changed, okay cheers, 10 minute read my ass. Not so fast; a lot indeed has changed, albeit under the hood of those old school development pillars.\nSome of the things that have changed are:\nHow we safeguard the code Where the code is executed How the code is built and moved How we plan How we manage to burnout developers faster How we care about the code we write How we safeguard the code Many, many different ways exist to version control your code; the following is a list of things that I have experienced and that I don‚Äôt recommend you try:\nA shared folder contains all source files Everyone has an offline copy Visual Source Safe on a shared folder StarTeam TFS over VSS TFS before they adopted git CVS Subversion in large teams Google Drive folder with change tracking Source control is, and probably will always be, an issue ‚Äî until everyone wakes up and accepts that git is the only free version control you will probably ever need. I state this on the premise that you are tracking changes in standard human-readable programming language source files, not large ML data sets, and that you don‚Äôt store all your source code in a single repository. I‚Äôm looking at you, Google .\nI‚Äôm glad to report that I have not experienced any source code not stored in some flavour of git in the past ten years, so it appears that the world is waking up to decent free version control.\nAlong with the adoption of git came all the cloud-based git offerings ‚Äî with GitHub being the biggest and most famous of them. With this git in the cloud business, developers can work from anywhere; they don‚Äôt need to be in the office or have VPN access. They can work from where and when they want and how they want. Finally, coding half-naked, eating Cheetos on your couch while watching Sam \u0026amp; Max can be a thing ‚Äî if that is what you are into.\nWhere the code is executed Back in the day, we had to guess specifications for a physical server, and we had to care about the CPU, RAM, HDDs and even the Network cards. This server often sat at the customer‚Äôs premises running on their infrastructure with their BOFH‚Äôs controlling it. If you wrote okay code and your Systems Architect, the guy that specced the server, knew their stuff, the post-prod deployment was a nice place to be. ü§£ü§£ü§£ That seldom happened! Either the server was too small, the interwebs feeding the server was too small, the raid controller sucked, the code sucked etc. Anything that required more infrastructure (server, network, switches, internet bandwidth) had massive lead times. Getting a new server normally took 6+weeks. So the only way to fix the epic levels of suckitude was to run the following code in a loop for a couple of weeks:\n1 2 3 for(customer.Cry()) { team.Optimize(); } Most of the code we write nowadays runs in the cloud. The cloud is just a Hardware Abstraction Layer; you know you want 1vCPU and 2GB of RAM, and you have stopped caring if it runs a Xeon, Epyc, ECC Memory, Optane SSD, SATA, SCSI or SAS. All the specifics are now gone. If it sucks, stop the instance, change the instance type, start it up again, rinse and repeat. Rinsing and repeating is not the answer to every prod problem; sometimes, the code is so wrong that throwing more hardware at it won‚Äôt make it better, and you still have to go back to the optimise loop described above.\nAn even more significant benefit to executing the code in the cloud is the capability of having distributed development teams. These teams can all have a development and staging environment running the latest code change you pushed five minutes ago, without every UI developer needing to pull the newest backend changes, compiling and running them locally. Having distributed development teams means that teams collaborate better and faster. When you find a bug in UI or backend, you can raise it immediately before the dev moves to the next ticket in the backlog. I‚Äôm aware this model doesn‚Äôt work for all types of development, but it has done wonders for web and mobile app development to speed up development.\nHow the code is built and moved A senior developer used to compile the code in Release mode with optimisations tuned to the production server‚Äôs CPU and architecture, zip the binaries, copy the archive to a network folder and finally ssh or RDP into the production environment and manually perform the release. Since this process is prone to hit-by-a-bus failure, a new buzzword-filled section in software development was born. It is called DevOps and basically, what this entails is that an automated process builds, tests, packages and even deploys it. There are so many different tools to cater for almost all your needs; you can build on an on-prem server and deploy on-prem or build in the cloud and deploy wherever you want; the options are only limited to your imagination. The main drive behind this is to remove the hit-by-a-bus risk and, more importantly, increase the speed at which you can get new/fixed code into an environment.\nThis field is not without its problems; if you build in the cloud and Azure DevOps, GitHub Actions, or AWS is experiencing an outage, you can become stuck with no option to deploy your code until the outage is restored.\nThe other, more critical, problem with DevOps, in general, is security. Yup, you take company IP and move it into a non-company controlled infrastructure optionally using sensitive data (config, keys, connection strings etc.) to perform the testing, building and deployments. Security is such a significant oversight that big consulting houses are called upon to perform DevSecOps audits.\nProbably the most impactful change DevOps has brought to the table, besides making some snowflake god-complex devs obsolete, is the exponential increase in unit/integration tests adopted by dev teams. To have confidence that the new code won‚Äôt break the perfectly working system, we write tests. These tests are executed as part of the DevOps pipeline. Only if these tests pass successfully are the built artefacts distributed. If the built artefacts are buggy as hell, developers WILL ENHANCE the test suite with more tests and the process repeats. We have come to a point where standard tests written by humans aren‚Äôt good enough for some people, and then they introduce chaos during testing, using fuzzers (see sharpfuzz and gofuzz ). Introducing chaos during testing should not be confused with chaos engineering, which is something way more fun.\nI have stopped counting the number of times tests have saved my new yet retarded code from destroying a perfectly working system. I don‚Äôt wonder if tests are worth it; I know they are, and I continue writing more. I have changed my attitude towards bug fixing from one of finding, fixing and manual testing to one of writing a test to verify the bug and then changing the code until the test passes.\nThis shift to automated testing has caused testing teams to be smaller and more efficient; however, it has caused some of them to lose their jobs. They will probably point and laugh at us when GitHub Copilot puts us out of a job.\nHow we plan In my experience, a Project Manager usually gets assigned the task of making developers build a poorly scoped system based on very bad thumb-sucked estimations and flawed assumptions. Waterfall, how I don‚Äôt miss thee.\nUnfortunately, the PM role still exists, but fortunately, the devs have revolted and demanded that we want more realistic thumb-sucks. Developers‚Äô demands for better estimations resulted in the birth of Agile and Agile-like methodologies that conned the PM into believing that smaller usable deliverables every fortnight will deliver the final product faster.\nHowever, this has forced devs to be more conscious about the planning process. Don‚Äôt think that Agile has solved the devs-are-lazy-af problem for one minute. I have on numerous occasions seen senior devs lying, saying something will take them four hours to do when in fact, that same thing will take a greenhorn thirty minutes to do correctly. I feel the shift to Agile-like methodologies has made devs more pedantic about how they break down and scope large complex tasks. By reviewing the work at the end of the sprint to see what was scoped correctly or incorrectly and learning from that, it creates a positive feedback cycle that helps the dev become more reliable, accountable and even better at scoping.\nAgile is a double-edged sword that takes care of inaccurate estimations and causes costs to align with actual effort. All roleplayers now have a better understanding of what they are getting themselves into beforehand and throughout. By no means has Agile methodologies made the estimates 100% accurate; they are mostly still off by some margin, but it is better than what it was.\nHow we manage to burnout developers faster The same tools we conned the PMs into using are also the cause for a steady increase in dev burnout. With Agile methodologies came a relentless pressure to deliver. The biggest culprit in the Agile world for this burnout is Sprints, and the clue to this burnout is in the name. A project comprises of N Sprints of Q weeks each. A Sprint starts typically on a Monday and ends Q weeks later on the Friday with a technical demo, Sprint planning and maybe a retrospective. That is, if there are no production bugs or testing bugs or unforeseen outages. All this pressure continues week after week, with no downtime, unless you go on leave or the project ends, where the whole process starts over again.\nIn toxic/unforgiving environments where the PM demands that you deliver on promised tasks each sprint, devs work longer and longer hours. Developers are punished for either bad planning, scoping or bugs they made in previous sprints. When under pressure, developers introduce even more bugs because they are overworked and burning out. To me, this scenario sounds very similar to thermal runaway.\nIn the days of the Waterfall SDLC, once development was completed, the project entered a lengthy testing period. Planning and design for new components scheduled for the next release cycle happened in this testing period. Now and again, developers would be interrupted by bugs found during testing, but in general, this time was stress-free and allowed developers to recharge.\nHow we care about the code we write In the past, only people with a bone to pick ran static code analysis tools and told you that some part of the system you wrote had a Cyclomatic complexity greater than 25 or that the Coupling was too high. Great, those things are ‚Äúimportant‚Äù, but you only knew about them when someone actively chose to perform those checks.\nMore than ever before, we now have to care more about the quality of the code that we write. Not a single week goes by without reading about a company that has been hacked or had a data leak. And 9.999 out of 10 times, the primary cause for this is shitty code written by a dev that swears his code was peer-reviewed, that the snippet on StackOverflow had 2000+ upvotes and posted by none other than Jon Skeet.\nThese days, we have many tools at our disposal to help us find and address these things while we develop the code (okay, not coupling and cohesion; experience and SOLID design principles are the only fixes for those). The many tools at our disposal are, to name but a few: linters, automatic code analysers (C#‚Äôs Roslyn Compiler), exceptional race detection in Go, GitHub‚Äôs security advisories, GitHub‚Äôs dependabot and tools like SonarQube. All these tools, that are primarily free, are helping devs write better and safer code. Because these tools constantly update, code that was fine yesterday can be flagged as vulnerable today due to some buffer overflow exploit in a 3rd party library that you are using. You can choose to act on it or ignore it, but the power is in your hands. I have been on the receiving end of a couple of scathing penetration testing reports in my days. Still, since the advent of most of the tools listed above in the projects I have been part of, I no longer dread penetration tests. I welcome them. These tools are not pointing out my shitty code; they highlight weaknesses in the system, like misconfigured gateways, etc.\nThe Open-source movement has also caused developers to care more about the quality of their code. Making one‚Äôs code publicly visible is a vulnerable thing to do. Other people look at it and at times ridicule ü§£ü§£ü§£ you thought I might say admire. Though it might be hard to do, posting code publically builds a community that care about similar things together to make even bigger and better things. And who benefits from this collaboration? Everyone! Previously most things were closed-sourced and required a license; developers added features slower than humanity has added landers on Mars, and bugs were only fixed if enough people complained.\nSo don‚Äôt be that guy that hides all his code in private repos on bitbucket, because of GPT-3. Take the leap and make your code public, share it with others. The worst thing that can happen is that someone will make a PR to correct your bad spelling. I don‚Äôt believe that it is my fault my alter ego is Typo Man!, the writer of wrongs.\n","permalink":"https://dbreedt.github.io/posts/changes_changes_2021/","summary":"\u003cp\u003eAn adage goes: ‚Äú\u003cem\u003eThe more things change, the more they stay the same\u003c/em\u003e‚Äù ‚Äî that kind of holds true for Software Development.\u003c/p\u003e\n\u003cp\u003eBack in the day, we used to design our systems, write the code, compile it, run it, get it tested and then deploy it. We still do all of that today, regardless of what we are building, be it microservices, large scale monolith systems, mobile apps, websites or firmware for IOT things.\u003c/p\u003e","title":"Changes, Changes everywhere"}]